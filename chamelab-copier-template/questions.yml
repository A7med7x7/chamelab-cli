# PROJECT CONFIGURATION

project_name:
  type: str
  default: mltrain-project
  help: |
    Project name that will be used for:
    - S3 bucket prefixes (e.g., project-name-data, project-name-mlflow-metrics)
    - Resource naming conventions
    - Directory and file organization
  validator: >-
    {% if not (project_name | regex_match("^[a-z][a-z0-9-]*[a-z0-9]$")) %}
    Project name must start with a lowercase letter, contain only lowercase letters, numbers, and hyphens, 
    and end with an alphanumeric character. No special characters allowed (for S3 compatibility).
    {% endif %}

author_name:
  type: str
  default: CC
  help: Your name or organization/team for the project

description:
  type: str
  default: AI project scaffolded for Chameleon Cloud
  help: A short description of your project

workflow:
  type: str
  choices:
    - resource-management
    - environment-setup
    - full-stack
  default: full-stack
  help: |
    Which workflow do you want to set up?
    - resource-management: Only Chameleon Cloud resource provisioning (notebooks for creating buckets, servers, cleanup)
    - environment-setup: Only Docker environment setup (compose files, Dockerfiles, scripts)
    - full-stack: Complete workflow with both resource management and environment setup


# RESOURCE MANAGEMENT QUESTIONS (conditional on workflow)

chameleon_site:
  type: str
  choices:
    - CHI@TACC
    - CHI@UC
  default: CHI@TACC
  help: |
    Chameleon Cloud site where your resources are located.
    - CHI@TACC: Texas Advanced Computing Center (most GPU nodes)
    - CHI@UC: University of Chicago
  when: "{{ workflow in ['resource-management', 'full-stack'] }}"

lease_name:
  type: str
  help: |
    Your Chameleon Cloud lease name.
    This should match the lease you created in the Chameleon Cloud portal.
    Example: "my-gpu-lease-2024"
  when: "{{ workflow in ['resource-management', 'full-stack'] }}"

gpu_type:
  type: str
  choices:
    - nvidia
    - amd
    - none
  default: nvidia
  help: GPU type you plan to use for your lease
  when: "{{ workflow in ['environment-setup', 'resource-management', 'full-stack'] }}"

# ENVIRONMENT SETUP QUESTIONS (conditional on workflow)

ml_framework:
  type: str
  choices:
    - pytorch
    - tensorflow
    - data-science
    - custom
  default: pytorch
  help: |
    Primary ML framework for your environment:
    - pytorch: PyTorch with CUDA/ROCm support
    - tensorflow: TensorFlow with GPU support
    - data-science: General data science stack (pandas, sklearn, etc.)
    - custom: Minimal base setup; you'll add your own requirements
  when: "{{ workflow in ['environment-setup', 'full-stack'] }}"

include_huggingface:
  type: bool
  default: true
  help: |
    Include HuggingFace token configuration?
    Required for downloading models from HuggingFace Hub.
    Token will be prompted during environment setup.
  when: "{{ workflow in ['environment-setup', 'full-stack'] }}"

include_example_notebooks:
  type: bool
  default: true
  help: |
    Include example ML notebooks showing:
    - PyTorch manual logging with MLflow
    - PyTorch Lightning auto-logging
    - Sklearn auto-logging
    - TensorFlow auto-logging
  when: "{{ workflow in ['environment-setup', 'full-stack'] }}"
